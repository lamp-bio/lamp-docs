# Rubric

## Rubrics <a id="rubrics"></a>

Annotations contain two parts: a priority and outline-style highlights.  
Therefore there are two rubrics.

### Highlights Rubric

* 3-5 bullets
* 85 character limit, including whitespace
* no end punctuation

#### Examples

* [Cancer Cell, Volume 32, Issue 2, 14 August 2017, Pages 169-184.e7](http://www.sciencedirect.com/science/article/pii/S1535610817302970)
  * Metastases mostly disseminate late from primary breast tumors, keeping most drivers
  * Drivers at relapse sample from a wider range of cancer genes than in primary tumors
  * Mutations in SWI-SNF complex and inactivated JAK-STAT signaling enriched at relapse
  * Mutational processes similar in primary and relapse; radiotherapy can damage genome
* [Learning and Instruction, Volume 21, Issue 6, December 2011, 746-756](http://www.sciencedirect.com/science/article/pii/S0959475211000387)
  * Fading of a script alone does not foster domain-general strategy knowledge
  * Performance of the strategy declines during the fading of a script
  * Monitoring by a peer keeps performance of the strategy up during script fading
  * Performance of a strategy after fading fosters domain-general strategy knowledge
  * Fading and monitoring by a peer combined foster domain-general strategy knowledge

### Priorities Rubric

{% hint style="info" %}
#### **Save Your Time!**

The Priorities Rubric is simply the sum of tedious details regarding how we:

1. Identified which references to annotate, and
2. Distributed those items with scores on a 1-3 scale, in a matter we found reasonable.

So, full disclosure: this information is under-documented for now.  
It's also only relevant for internal purposes at this point, if at all.
{% endhint %}

{% hint style="success" %}
#### **Eventually, post-publication**

Code, an interactive citation graph, and other goodies used in this process will appear.
{% endhint %}



#### Preparation

* Each reference contains provenance tagging by section.
* This `group` field allows us to analyze our citations.
* We fetched further informative metadata via:
  1. first-party literature analysis,
  2. third-party machine-learning platforms, and
  3. third-party query services such as Crossref.

#### Calculation

We then tabulated two levels of citation analysis:

1. First, we looked at descriptive statistics.[¹](./#1)
2. Next, we calculated a few nontrivial measures.[²](./#2)
3. Finally, we did a qualitative rundown of known-significant works and how we cited them.

#### Descriptive Stats

Descriptive stats of relevance include:

1. citation frequency,
2. citation centrality,
3. between-section differences, and so forth.

#### Statistical Measures

Statistical measures of relevance include:

1. mutual and interaction information,
2. graphical properties — especially cliques and skew partitions,
3. "standard" citation analysis benchmarks, and so forth.
4. Finally, we 

* This ultimately the combined multi-bibliography by factors like:
* * citation frequency,
  * citation centrality, and
  * citation mutual / interaction information wit

## Evolution

{% hint style="warning" %}
This section is a stub.  
Notes on how we arrived at these outcomes to appear.
{% endhint %}

